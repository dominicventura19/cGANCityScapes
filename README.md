# Objective
The purpose of this notebook was to learn and understand image to image translation using conditional GANs (cGANs). This concept was described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004).

Note: GANs can take a long time to train. I did 150 epochs on my school's server that has a GPU and each epoch took roughly 2 minutes.

Dataset used can be found [here](https://www.kaggle.com/vikramtiwari/pix2pix-dataset).

Plots of the generator and discriminator losses can be viewed [here](https://tensorboard.dev/experiment/HnZe4oRhRBi3oK98evaKLg/#scalars).

#

# What is Image-to-Image Translation

Image-to-Image translation is a class of vision and graphics problems where the goal is to learn the mapping between from a source image X to target Y such that the distribution of X is indistinguishable from Y. This can be applied to a wide range of problems such as style transfer, photo enhancement, etc....  

Here we use a conditional GAN (cGAN) to do this mapping. A cGAN learns the mapping and also lears a loss function to train this mapping. This makes it possible to apply some generic approach to problems that usually require very different loss functions. 

# Short Introduction to GANs

A generative adverserial network (GAN) is a class of machine learning frameworks that is comprised of two neural networks: a generator and a discriminator.

The generator generates some random noise (because the weights are random) and generates an image as output.
![](https://github.com/dominicventura19/cGANCityScapes/blob/main/slide_photos/generator.png)

The discriminator tries to distinguish between the real image and the fake image generated by the generator. The output from the discriminator is a probability where values closer to one mean a higher probability that the image is real and vice versa. 
<p align="center">
  <img width="700" height="100" src="https://github.com/dominicventura19/cGANCityScapes/blob/main/slide_photos/discrim_1.png">
</p>

<p align="center">
  <img width="700" height="100" src="https://github.com/dominicventura19/cGANCityScapes/blob/main/slide_photos/discrim_2.png">
</p>


Latent samples are fed to the GAN while we set the expected outcome (label) to 1 (real) as we expect the generator to produce realistic looking images and we expect the discriminator to say it's real. But, the generator will initially produce a bad or blurry image and the loss value will be high (this is because the weights are random). So, back-propogation updates the generator's weights to produce more realistic looking images as the training continues. This is how the generator is trained. 

<p align="center">
  <img width="800" height="400" src="https://github.com/dominicventura19/cGANCityScapes/blob/main/slide_photos/gan_overall.png">
</p>

Overall view of how a GAN works.

# cGAN

A limitation of the GANs is that it can generate a random image from the domain (train set), which we have no control over. There is a complex relationship between the points in the latent space to the generated image that is difficult to map. 

Alternatively, a GAN can be trained in such a way that both the generator and discriminator are conditioned on the class label. This means that when the trained generator model is used as a standalone model to generate images in the domain, images of a given class label can be generared. This is a cGAN.

### How It Works

Before, images were being fed to both the generator and discriminator as the only input, but now we will be feeding class information to both networks. Now, the generator takes random noise and a one-hot encoded class label as input and outputs a fake image of a particular class. Then, the discriminator takes an image with one-hot labels added as depth to the image (channels) i.e. if you have an image of 28x28x1 size and a one-hot vector of size n, then the image size will be 28x28x(n+1). Finally, the disciminator outputs whether the image belongs to that class or not, i.e. real or fake.

# Dataset

The dataset was comprised of about 3,000 segmented trinaing images and 500 testing images and each image was 256x256 in size.

# Training Images

For each epoch, I saved the training image and the predicited image and combined all 150 into a gif so the progress from epoch 1 to epoch 150 could be seen:


![](https://github.com/dominicventura19/cGANCityScapes/blob/main/half_speed.gif)

# Test Images

Below are the test images generated by the cGAN: 

![](https://github.com/dominicventura19/cGANCityScapes/blob/main/Images/test_0.png)
![](https://github.com/dominicventura19/cGANCityScapes/blob/main/Images/test_1.png)

# Losses

At the beginning, I provided a link to the loss plots on TensorBoard. Some things to note when looking at them to help interpret:

1) Checking if gen_gan_loss or disc_loss gets very low. This is an indicator that one of the models is dominating the other and training isn't successful.

2) The value log(2) = 0.69 is a good reference point for these losses. This indicates the discriminator is on average equally unsure about the two options (real or fake image). 

3) For the discriminator loss, values below 0.69 means the discriminator is doing better than random at choosing which image is the generated image.

4) For gen_gan_loss, values above 0.69 means the generator is doing better than random at fooling the discriminator.

5) Overall, as training goes on, gen_l1_loss should go down.
